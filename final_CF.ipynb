{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "target_user_id=pd.read_csv('/project/at081-group38/AT081150_AN/193_ER/target_user_id.csv')\n",
    "validation_test_data=pd.read_csv('/project/at081-group38/AT081150_AN/193_ER/validation_testing_data.csv')\n",
    "final_final=pd.read_csv('/project/at081-group38/AT081150_AN/193_ER/final_final.csv')\n",
    "df_news_all=pd.read_csv('/project/at081-group38/AT081150_AN/193_ER/df_news_all.csv')\n",
    "\n",
    "from scipy import spatial\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import re\n",
    "import os\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import pickle\n",
    "\n",
    "from gensim.models import Doc2Vec, doc2vec\n",
    "from gensim.models import word2vec\n",
    "import random\n",
    "import logging\n",
    "\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_predict_output=pd.read_csv('/project/at081-group38/AT081150_AN/193_ER/project-BERt/bert/tmp/news_output/test_output/test_results.tsv',sep='\\t')\n",
    "bert_test_data_with_target = pd.read_csv('/project/at081-group38/AT081150_AN/193_ER/bert_test_data_with_target.csv')\n",
    "#取得變成columns name的資料\n",
    "temp=bert_predict_output.columns.values\n",
    "\n",
    "#轉成dataFrame\n",
    "d = {'notClick':[temp[0]], 'Click':[temp[1]]} \n",
    "first_df=pd.DataFrame(d)\n",
    "\n",
    "#將原本的欄位重新命名\n",
    "bert_predict_output.columns=['notClick','Click']\n",
    "\n",
    "#把兩個資料合併，注意 concat的第一個資料集須放變成欄位名稱的資料，不然位置會錯\n",
    "bert_predict_output=pd.concat([first_df, bert_predict_output],ignore_index=True)\n",
    "bert_predict_output.head()\n",
    "\n",
    "#把整理好的資料再跟bert_test_data水平合併\n",
    "bert_result = pd.concat([bert_test_data_with_target, bert_predict_output],axis=1)\n",
    "bert_result = bert_result.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result[[\"notClick\", \"Click\"]] = bert_result[[\"notClick\", \"Click\"]].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 這裡暫不使用\n",
    "# target_user_id=target_csv.iloc[0,:].values[0]\n",
    "# pred_time = validation_test_data[validation_test_data.user_id == target_user_id].iloc[0,:].time\n",
    "\n",
    "# bert_result_date = bert_result.date\n",
    "# for i in range(len(bert_result)):\n",
    "#     if (pred_time-bert_result_date[i])>200000000: # 若該新聞發布日期和推薦時間超過兩個月則刪除該候選新聞\n",
    "#         bert_result = bert_result.drop([i])\n",
    "# bert_result = bert_result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result = bert_result.sort_values(by=\"Click\" , ascending=False)\n",
    "bert_result = bert_result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>date</th>\n",
       "      <th>news_guid</th>\n",
       "      <th>title</th>\n",
       "      <th>news_content</th>\n",
       "      <th>content_remove_html_tag</th>\n",
       "      <th>notClick</th>\n",
       "      <th>Click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/news/2018-05-09/7853913e-eaf7-4c8d...</td>\n",
       "      <td>20180509085858</td>\n",
       "      <td>7853913E-EAF7-4C8D-A21F-2281A02E5DA5</td>\n",
       "      <td>精華Q1 EPS 7元；新產能陸續投產為今年營運添動能                   ...</td>\n",
       "      <td>\\n&lt;P&gt;MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導&lt;...</td>\n",
       "      <td>MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導隱形眼鏡大廠...</td>\n",
       "      <td>0.407336</td>\n",
       "      <td>0.592664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/news/2018-02-01/4cbfc6ab-2526-4b87...</td>\n",
       "      <td>20180201133759</td>\n",
       "      <td>4CBFC6AB-2526-4B87-85BE-7B646A214BA7</td>\n",
       "      <td>環球晶：半導體矽晶圓仍有漲價空間，今年拚逐季成長                      ...</td>\n",
       "      <td>\\n&lt;P&gt;MoneyDJ新聞 2018-02-01 13:37:59 記者 新聞中心 報導&lt;...</td>\n",
       "      <td>MoneyDJ新聞 2018-02-01 13:37:59 記者 新聞中心 報導半導體矽晶圓...</td>\n",
       "      <td>0.408350</td>\n",
       "      <td>0.591650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path            date  \\\n",
       "0  ../datasets/news/2018-05-09/7853913e-eaf7-4c8d...  20180509085858   \n",
       "1  ../datasets/news/2018-02-01/4cbfc6ab-2526-4b87...  20180201133759   \n",
       "\n",
       "                              news_guid  \\\n",
       "0  7853913E-EAF7-4C8D-A21F-2281A02E5DA5   \n",
       "1  4CBFC6AB-2526-4B87-85BE-7B646A214BA7   \n",
       "\n",
       "                                               title  \\\n",
       "0  精華Q1 EPS 7元；新產能陸續投產為今年營運添動能                   ...   \n",
       "1  環球晶：半導體矽晶圓仍有漲價空間，今年拚逐季成長                      ...   \n",
       "\n",
       "                                        news_content  \\\n",
       "0  \\n<P>MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導<...   \n",
       "1  \\n<P>MoneyDJ新聞 2018-02-01 13:37:59 記者 新聞中心 報導<...   \n",
       "\n",
       "                             content_remove_html_tag  notClick     Click  \n",
       "0  MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導隱形眼鏡大廠...  0.407336  0.592664  \n",
       "1  MoneyDJ新聞 2018-02-01 13:37:59 記者 新聞中心 報導半導體矽晶圓...  0.408350  0.591650  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1651"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### 選擇分數最高的前 n 筆\n",
    "n = 1000\n",
    "####################\n",
    "bert_result = bert_result.drop(bert_result.index.values[(bert_result.index.values>n-1).tolist()].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目標已存在候選集合當中\n"
     ]
    }
   ],
   "source": [
    "target_user_id_str=target_user_id.iloc[0,:].values[0]\n",
    "validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "if bert_result[bert_result.news_guid==validation_data_guid].empty:\n",
    "    print('目標不存在候選集合當中')\n",
    "\n",
    "else:\n",
    "    print('目標已存在候選集合當中')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將最後一筆觀看紀錄加入資料當中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>news_id</th>\n",
       "      <th>C011010</th>\n",
       "      <th>C011011</th>\n",
       "      <th>C011020</th>\n",
       "      <th>C011021</th>\n",
       "      <th>C011022</th>\n",
       "      <th>C011023</th>\n",
       "      <th>C011024</th>\n",
       "      <th>...</th>\n",
       "      <th>C099391</th>\n",
       "      <th>C099392</th>\n",
       "      <th>C099393</th>\n",
       "      <th>C099394</th>\n",
       "      <th>C099395</th>\n",
       "      <th>C099396</th>\n",
       "      <th>C099397</th>\n",
       "      <th>C099400</th>\n",
       "      <th>C099410</th>\n",
       "      <th>C099411</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180516222843</td>\n",
       "      <td>00039bc1-c70c-404d-9a16-5ec2227061f7</td>\n",
       "      <td>67172077-4536-405A-B5FD-E7793171C5E8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180516223042</td>\n",
       "      <td>00039bc1-c70c-404d-9a16-5ec2227061f7</td>\n",
       "      <td>8C789423-6574-4BAA-A50A-7B3F9C25C8B8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 898 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             time                               user_id  \\\n",
       "0  20180516222843  00039bc1-c70c-404d-9a16-5ec2227061f7   \n",
       "1  20180516223042  00039bc1-c70c-404d-9a16-5ec2227061f7   \n",
       "\n",
       "                                news_id  C011010  C011011  C011020  C011021  \\\n",
       "0  67172077-4536-405A-B5FD-E7793171C5E8        0        0        0        0   \n",
       "1  8C789423-6574-4BAA-A50A-7B3F9C25C8B8        0        0        0        0   \n",
       "\n",
       "   C011022  C011023  C011024   ...     C099391  C099392  C099393  C099394  \\\n",
       "0        0        0        0   ...           0        0        0        0   \n",
       "1        0        0        0   ...           0        0        0        0   \n",
       "\n",
       "   C099395  C099396  C099397  C099400  C099410  C099411  \n",
       "0        0        0        0        0        0        0  \n",
       "1        0        0        0        0        0        0  \n",
       "\n",
       "[2 rows x 898 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_final = final_final.loc[:,'time':'news_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出該 user 最後一筆看的 news\n",
    "particular_user_last_news_id = final_final[final_final.user_id == target_user_id_str].tail(1).news_id.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>date</th>\n",
       "      <th>news_guid</th>\n",
       "      <th>title</th>\n",
       "      <th>news_content</th>\n",
       "      <th>content_remove_html_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/news/1999-06-14/0b7a5d15-7bcd-11d3...</td>\n",
       "      <td>19990614074852</td>\n",
       "      <td>0B7A5D15-7BCD-11D3-98FF-00E018A00403</td>\n",
       "      <td>導線架市場重新洗牌新台和順德談策略聯盟                           ...</td>\n",
       "      <td>\\n\\n\\n&lt;P&gt;在大廠壓縮小廠生存空間，小廠試圖尋求同業策略聯盟的趨勢下，國內3大導線架廠...</td>\n",
       "      <td>在大廠壓縮小廠生存空間，小廠試圖尋求同業策略聯盟的趨勢下，國內3大導線架廠包括順德、佳茂、旭...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/news/2009-08-06/3b508922-e6a5-4c25...</td>\n",
       "      <td>20090806111315</td>\n",
       "      <td>3B508922-E6A5-4C25-B80F-01E8B36BB835</td>\n",
       "      <td>建築鋼每噸利潤達1000元 中國鐵礦石庫存逼新高                      ...</td>\n",
       "      <td>\\n&lt;P&gt;精實新聞 2009-08-06 11:13:15 記者 余美慧 報導&lt;/P&gt;&lt;P&gt;...</td>\n",
       "      <td>精實新聞 2009-08-06 11:13:15 記者 余美慧 報導聯合金屬網統計，截至7月...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path            date  \\\n",
       "0  ../datasets/news/1999-06-14/0b7a5d15-7bcd-11d3...  19990614074852   \n",
       "1  ../datasets/news/2009-08-06/3b508922-e6a5-4c25...  20090806111315   \n",
       "\n",
       "                              news_guid  \\\n",
       "0  0B7A5D15-7BCD-11D3-98FF-00E018A00403   \n",
       "1  3B508922-E6A5-4C25-B80F-01E8B36BB835   \n",
       "\n",
       "                                               title  \\\n",
       "0  導線架市場重新洗牌新台和順德談策略聯盟                           ...   \n",
       "1  建築鋼每噸利潤達1000元 中國鐵礦石庫存逼新高                      ...   \n",
       "\n",
       "                                        news_content  \\\n",
       "0  \\n\\n\\n<P>在大廠壓縮小廠生存空間，小廠試圖尋求同業策略聯盟的趨勢下，國內3大導線架廠...   \n",
       "1  \\n<P>精實新聞 2009-08-06 11:13:15 記者 余美慧 報導</P><P>...   \n",
       "\n",
       "                             content_remove_html_tag  \n",
       "0  在大廠壓縮小廠生存空間，小廠試圖尋求同業策略聯盟的趨勢下，國內3大導線架廠包括順德、佳茂、旭...  \n",
       "1  精實新聞 2009-08-06 11:13:15 記者 余美慧 報導聯合金屬網統計，截至7月...  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_all.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "particular_user_last_news = df_news_all[df_news_all.news_guid == particular_user_last_news_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>date</th>\n",
       "      <th>news_guid</th>\n",
       "      <th>title</th>\n",
       "      <th>news_content</th>\n",
       "      <th>content_remove_html_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46682</th>\n",
       "      <td>../datasets/news/2018-08-27/6985fa32-38fa-4c03...</td>\n",
       "      <td>20180827081149</td>\n",
       "      <td>6985FA32-38FA-4C03-98E5-103CD6B488D7</td>\n",
       "      <td>元太：H2營收優於H1；全年毛利率拚重回40%之上                     ...</td>\n",
       "      <td>\\n&lt;P&gt;MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導&lt;...</td>\n",
       "      <td>MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導元太(806...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path            date  \\\n",
       "46682  ../datasets/news/2018-08-27/6985fa32-38fa-4c03...  20180827081149   \n",
       "\n",
       "                                  news_guid  \\\n",
       "46682  6985FA32-38FA-4C03-98E5-103CD6B488D7   \n",
       "\n",
       "                                                   title  \\\n",
       "46682  元太：H2營收優於H1；全年毛利率拚重回40%之上                     ...   \n",
       "\n",
       "                                            news_content  \\\n",
       "46682  \\n<P>MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導<...   \n",
       "\n",
       "                                 content_remove_html_tag  \n",
       "46682  MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導元太(806...  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particular_user_last_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result = bert_result.drop(['notClick', 'Click'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>date</th>\n",
       "      <th>news_guid</th>\n",
       "      <th>title</th>\n",
       "      <th>news_content</th>\n",
       "      <th>content_remove_html_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/news/2018-05-09/7853913e-eaf7-4c8d...</td>\n",
       "      <td>20180509085858</td>\n",
       "      <td>7853913E-EAF7-4C8D-A21F-2281A02E5DA5</td>\n",
       "      <td>精華Q1 EPS 7元；新產能陸續投產為今年營運添動能                   ...</td>\n",
       "      <td>\\n&lt;P&gt;MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導&lt;...</td>\n",
       "      <td>MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導隱形眼鏡大廠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/news/2018-02-01/4cbfc6ab-2526-4b87...</td>\n",
       "      <td>20180201133759</td>\n",
       "      <td>4CBFC6AB-2526-4B87-85BE-7B646A214BA7</td>\n",
       "      <td>環球晶：半導體矽晶圓仍有漲價空間，今年拚逐季成長                      ...</td>\n",
       "      <td>\\n&lt;P&gt;MoneyDJ新聞 2018-02-01 13:37:59 記者 新聞中心 報導&lt;...</td>\n",
       "      <td>MoneyDJ新聞 2018-02-01 13:37:59 記者 新聞中心 報導半導體矽晶圓...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path            date  \\\n",
       "0  ../datasets/news/2018-05-09/7853913e-eaf7-4c8d...  20180509085858   \n",
       "1  ../datasets/news/2018-02-01/4cbfc6ab-2526-4b87...  20180201133759   \n",
       "\n",
       "                              news_guid  \\\n",
       "0  7853913E-EAF7-4C8D-A21F-2281A02E5DA5   \n",
       "1  4CBFC6AB-2526-4B87-85BE-7B646A214BA7   \n",
       "\n",
       "                                               title  \\\n",
       "0  精華Q1 EPS 7元；新產能陸續投產為今年營運添動能                   ...   \n",
       "1  環球晶：半導體矽晶圓仍有漲價空間，今年拚逐季成長                      ...   \n",
       "\n",
       "                                        news_content  \\\n",
       "0  \\n<P>MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導<...   \n",
       "1  \\n<P>MoneyDJ新聞 2018-02-01 13:37:59 記者 新聞中心 報導<...   \n",
       "\n",
       "                             content_remove_html_tag  \n",
       "0  MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導隱形眼鏡大廠...  \n",
       "1  MoneyDJ新聞 2018-02-01 13:37:59 記者 新聞中心 報導半導體矽晶圓...  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_with_user_last_news = pd.concat([particular_user_last_news, bert_result], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>date</th>\n",
       "      <th>news_guid</th>\n",
       "      <th>title</th>\n",
       "      <th>news_content</th>\n",
       "      <th>content_remove_html_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/news/2018-08-27/6985fa32-38fa-4c03...</td>\n",
       "      <td>20180827081149</td>\n",
       "      <td>6985FA32-38FA-4C03-98E5-103CD6B488D7</td>\n",
       "      <td>元太：H2營收優於H1；全年毛利率拚重回40%之上                     ...</td>\n",
       "      <td>\\n&lt;P&gt;MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導&lt;...</td>\n",
       "      <td>MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導元太(806...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/news/2018-05-09/7853913e-eaf7-4c8d...</td>\n",
       "      <td>20180509085858</td>\n",
       "      <td>7853913E-EAF7-4C8D-A21F-2281A02E5DA5</td>\n",
       "      <td>精華Q1 EPS 7元；新產能陸續投產為今年營運添動能                   ...</td>\n",
       "      <td>\\n&lt;P&gt;MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導&lt;...</td>\n",
       "      <td>MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導隱形眼鏡大廠...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path            date  \\\n",
       "0  ../datasets/news/2018-08-27/6985fa32-38fa-4c03...  20180827081149   \n",
       "1  ../datasets/news/2018-05-09/7853913e-eaf7-4c8d...  20180509085858   \n",
       "\n",
       "                              news_guid  \\\n",
       "0  6985FA32-38FA-4C03-98E5-103CD6B488D7   \n",
       "1  7853913E-EAF7-4C8D-A21F-2281A02E5DA5   \n",
       "\n",
       "                                               title  \\\n",
       "0  元太：H2營收優於H1；全年毛利率拚重回40%之上                     ...   \n",
       "1  精華Q1 EPS 7元；新產能陸續投產為今年營運添動能                   ...   \n",
       "\n",
       "                                        news_content  \\\n",
       "0  \\n<P>MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導<...   \n",
       "1  \\n<P>MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導<...   \n",
       "\n",
       "                             content_remove_html_tag  \n",
       "0  MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導元太(806...  \n",
       "1  MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導隱形眼鏡大廠...  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_result_with_user_last_news.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf92367885b42b4949218686e1e5f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "title=[]\n",
    "content=[]\n",
    "title_content=[]\n",
    "\n",
    "for index,row in tqdm_notebook(bert_result_with_user_last_news.iterrows()):\n",
    "    title.append(row['title'])\n",
    "    content.append(row['content_remove_html_tag'])\n",
    "    title_content.append(row['title']+' '+row['content_remove_html_tag'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove html tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 處理 title 的 html\n",
    "li = []\n",
    "for i in range(len(title)):\n",
    "    li.append(\" \".join(title[i].split()))\n",
    "title = li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 處理 content 的 html\n",
    "li = []\n",
    "for i in range(len(content)):\n",
    "    li.append(\" \".join(content[i].split()))\n",
    "content = li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 處理 content 的 html\n",
    "li = []\n",
    "for i in range(len(title_content)):\n",
    "    li.append(\" \".join(title_content[i].split()))\n",
    "title_content = li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp='\\n'\n",
    "title_s=sp.join(title)\n",
    "content_s=sp.join(content)\n",
    "title_content_s=sp.join(title_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/project/at081-group38/AT081150_AN/193_ER/title_s.txt\", \"w\") as text_file:\n",
    "    text_file.write(title_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/project/at081-group38/AT081150_AN/193_ER/content_s.txt\", \"w\") as text_file:\n",
    "    text_file.write(content_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/project/at081-group38/AT081150_AN/193_ER/title_content_s.txt\", \"w\") as text_file:\n",
    "    text_file.write(title_content_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################################################################\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layers=-1,-2,-3,-4 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Traceback (most recent call last):\n",
      "  File \"extract_features.py\", line 419, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
      "    _sys.exit(main(argv))\n",
      "  File \"extract_features.py\", line 360, in main\n",
      "    examples = read_examples(FLAGS.input_file)\n",
      "  File \"extract_features.py\", line 325, in read_examples\n",
      "    line = tokenization.convert_to_unicode(reader.readline())\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 183, in readline\n",
      "    self._preread_check()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 85, in _preread_check\n",
      "    compat.as_bytes(self.__name), 1024 * 512, status)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 519, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: /project/at081-group38/AT081193_ER/title_content_s.txt; No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python extract_features.py \\\n",
    "  --input_file=/project/at081-group38/AT081193_ER/title_content_s.txt \\\n",
    "  --output_file=tmp/output.jsonl \\\n",
    "  --vocab_file=chinese_L-12_H-768_A-12/vocab.txt \\\n",
    "  --bert_config_file=chinese_L-12_H-768_A-12/bert_config.json \\\n",
    "  --init_checkpoint=tmp/news_output/model.ckpt-18200.index \\\n",
    "  --layers=-1 \\\n",
    "  --max_seq_length=128 \\\n",
    "  --batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open('tmp/output.jsonl') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in range(len(data)):\n",
    "    result = 1 - spatial.distance.cosine(data[0]['features'][0]['layers'][0]['values'], data[i]['features'][0]['layers'][0]['values'])\n",
    "    li.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_bert_feature = pd.DataFrame(li)\n",
    "cosine_similarity_bert_feature.columns = ['cosine_similarity']\n",
    "cosine_similarity_bert_feature = cosine_similarity_bert_feature.sort_values(by = ['cosine_similarity'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_candidate_bert_feature = bert_result_with_user_last_news.iloc[cosine_similarity_bert_feature.index[1:400],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目標不存在候選集合當中\n"
     ]
    }
   ],
   "source": [
    "#如果不存在裡面的話，就把加入Validation_data\n",
    "validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "if final_candidate_bert_feature[final_candidate_bert_feature.news_guid==validation_data_guid].empty:\n",
    "    print('目標不存在候選集合當中')\n",
    "else:\n",
    "    print('目標已存在候選集合當中')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"../\") # 指定工作路徑\n",
    "# os.getcwd() # 返回當前工作路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set dictionary (can define yourself)\n",
    "jieba.set_dictionary('/project/at081-group38/AT081150_AN/193_ER/jieba/dict.txt.big')\n",
    "stop_words = open('/project/at081-group38/AT081150_AN/193_ER/jieba/stop_words.txt', encoding=\"utf-8\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter rules\n",
    "bert_result_with_user_last_news['content_remove_html_tag'] = bert_result_with_user_last_news['content_remove_html_tag'].str.replace('https?:\\/\\/\\S*', '')\n",
    "bert_result_with_user_last_news['content_remove_html_tag'] = bert_result_with_user_last_news['content_remove_html_tag'].replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove data\n",
    "bert_result_with_user_last_news = bert_result_with_user_last_news.dropna()\n",
    "bert_result_with_user_last_news = bert_result_with_user_last_news.reset_index(drop=True)\n",
    "bert_result_with_user_last_news['idx'] = bert_result_with_user_last_news.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>date</th>\n",
       "      <th>news_guid</th>\n",
       "      <th>title</th>\n",
       "      <th>news_content</th>\n",
       "      <th>content_remove_html_tag</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/news/2018-08-27/6985fa32-38fa-4c03...</td>\n",
       "      <td>20180827081149</td>\n",
       "      <td>6985FA32-38FA-4C03-98E5-103CD6B488D7</td>\n",
       "      <td>元太：H2營收優於H1；全年毛利率拚重回40%之上                     ...</td>\n",
       "      <td>\\n&lt;P&gt;MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導&lt;...</td>\n",
       "      <td>MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導元太(806...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/news/2018-05-09/7853913e-eaf7-4c8d...</td>\n",
       "      <td>20180509085858</td>\n",
       "      <td>7853913E-EAF7-4C8D-A21F-2281A02E5DA5</td>\n",
       "      <td>精華Q1 EPS 7元；新產能陸續投產為今年營運添動能                   ...</td>\n",
       "      <td>\\n&lt;P&gt;MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導&lt;...</td>\n",
       "      <td>MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導隱形眼鏡大廠...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path            date  \\\n",
       "0  ../datasets/news/2018-08-27/6985fa32-38fa-4c03...  20180827081149   \n",
       "1  ../datasets/news/2018-05-09/7853913e-eaf7-4c8d...  20180509085858   \n",
       "\n",
       "                              news_guid  \\\n",
       "0  6985FA32-38FA-4C03-98E5-103CD6B488D7   \n",
       "1  7853913E-EAF7-4C8D-A21F-2281A02E5DA5   \n",
       "\n",
       "                                               title  \\\n",
       "0  元太：H2營收優於H1；全年毛利率拚重回40%之上                     ...   \n",
       "1  精華Q1 EPS 7元；新產能陸續投產為今年營運添動能                   ...   \n",
       "\n",
       "                                        news_content  \\\n",
       "0  \\n<P>MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導<...   \n",
       "1  \\n<P>MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導<...   \n",
       "\n",
       "                             content_remove_html_tag  idx  \n",
       "0  MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導元太(806...    0  \n",
       "1  MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導隱形眼鏡大廠...    1  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_result_with_user_last_news.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_with_user_last_news = bert_result_with_user_last_news.drop(['path', 'news_content'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_with_user_last_news['title_content'] = bert_result_with_user_last_news[['content_remove_html_tag', 'title']].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_with_user_last_news.columns = ['date', 'news_guid', 'title', 'content', 'idx', 'title_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_with_user_last_news = bert_result_with_user_last_news.reindex(columns=['date', 'news_guid', 'title', 'content',  'title_content', 'idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>news_guid</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>title_content</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180827081149</td>\n",
       "      <td>6985FA32-38FA-4C03-98E5-103CD6B488D7</td>\n",
       "      <td>元太：H2營收優於H1；全年毛利率拚重回40%之上                     ...</td>\n",
       "      <td>MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導元太(806...</td>\n",
       "      <td>MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導元太(806...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180509085858</td>\n",
       "      <td>7853913E-EAF7-4C8D-A21F-2281A02E5DA5</td>\n",
       "      <td>精華Q1 EPS 7元；新產能陸續投產為今年營運添動能                   ...</td>\n",
       "      <td>MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導隱形眼鏡大廠...</td>\n",
       "      <td>MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導隱形眼鏡大廠...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date                             news_guid  \\\n",
       "0  20180827081149  6985FA32-38FA-4C03-98E5-103CD6B488D7   \n",
       "1  20180509085858  7853913E-EAF7-4C8D-A21F-2281A02E5DA5   \n",
       "\n",
       "                                               title  \\\n",
       "0  元太：H2營收優於H1；全年毛利率拚重回40%之上                     ...   \n",
       "1  精華Q1 EPS 7元；新產能陸續投產為今年營運添動能                   ...   \n",
       "\n",
       "                                             content  \\\n",
       "0  MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導元太(806...   \n",
       "1  MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導隱形眼鏡大廠...   \n",
       "\n",
       "                                       title_content  idx  \n",
       "0  MoneyDJ新聞 2018-08-27 08:11:49 記者 新聞中心 報導元太(806...    0  \n",
       "1  MoneyDJ新聞 2018-05-09 08:58:58 記者 新聞中心 報導隱形眼鏡大廠...    1  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_result_with_user_last_news.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先進行斷詞 (選擇 title、 content、 title_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可選擇 title、 content、 title_content\n",
    "content_data = bert_result_with_user_last_news['title_content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aea11ee69864e00ae5fba535920f93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /project/at081-group38/AT081150_AN/193_ER/jieba/dict.txt.big ...\n",
      "2019-07-04 09:39:00,537: DEBUG: Building prefix dict from /project/at081-group38/AT081150_AN/193_ER/jieba/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.ufc3a5e786f812e3e4ea9d2b74f1db274.cache\n",
      "2019-07-04 09:39:00,539: DEBUG: Loading model from cache /tmp/jieba.ufc3a5e786f812e3e4ea9d2b74f1db274.cache\n",
      "Loading model cost 1.217 seconds.\n",
      "2019-07-04 09:39:01,755: DEBUG: Loading model cost 1.217 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-07-04 09:39:01,756: DEBUG: Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "for i, text in enumerate(tqdm_notebook(content_data)):\n",
    "    line = []\n",
    "\n",
    "    for w in jieba.cut(text, cut_all=False):\n",
    "        \n",
    "        ## remove stopwords and digits\n",
    "        ## can define your own rules\n",
    "        if w not in stop_words and not bool(re.match('[0-9]+', w)):\n",
    "            line.append(w)\n",
    "\n",
    "    sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "cut_data = 'content_cutted'\n",
    "########################################################\n",
    "## save data as pickle format\n",
    "with open('/project/at081-group38/AT081150_AN/193_ER/'+ cut_data, \"wb\") as file:\n",
    "    pickle.dump(sentences, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通過 logging.basicConfig 函數對日誌的輸出格式及方式做相關的配置\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load 'content_cutted'\n",
    "with open('/project/at081-group38/AT081150_AN/193_ER/'+ cut_data, 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## average word vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word2vec\n",
    "# sg=0 CBOW ; sg=1 skip-gram\n",
    "model = word2vec.Word2Vec(size=256, min_count=5, window=5, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-04 09:39:19,647: INFO: collecting all words and their counts\n",
      "2019-07-04 09:39:19,648: INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-07-04 09:39:19,713: INFO: collected 25371 word types from a corpus of 338503 raw words and 1001 sentences\n",
      "2019-07-04 09:39:19,713: INFO: Loading a fresh vocabulary\n",
      "2019-07-04 09:39:19,742: INFO: min_count=5 retains 6695 unique words (26% of original 25371, drops 18676)\n",
      "2019-07-04 09:39:19,743: INFO: min_count=5 leaves 308303 word corpus (91% of original 338503, drops 30200)\n",
      "2019-07-04 09:39:19,759: INFO: deleting the raw counts dictionary of 25371 items\n",
      "2019-07-04 09:39:19,760: INFO: sample=0.001 downsamples 42 most-common words\n",
      "2019-07-04 09:39:19,760: INFO: downsampling leaves estimated 279954 word corpus (90.8% of prior 308303)\n",
      "2019-07-04 09:39:19,761: INFO: estimated required memory for 6695 words and 256 dimensions: 17058860 bytes\n",
      "2019-07-04 09:39:19,774: INFO: resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary\n",
    "model.build_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-04 09:39:19,871: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:20,118: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:20,122: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:20,128: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:20,128: INFO: training on 338503 raw words (280004 effective words) took 0.3s, 1107376 effective words/s\n",
      "2019-07-04 09:39:20,129: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:20,375: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:20,380: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:20,381: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:20,381: INFO: training on 338503 raw words (279816 effective words) took 0.2s, 1119748 effective words/s\n",
      "2019-07-04 09:39:20,383: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:20,621: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:20,626: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:20,634: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:20,634: INFO: training on 338503 raw words (279874 effective words) took 0.2s, 1122413 effective words/s\n",
      "2019-07-04 09:39:20,636: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:20,877: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:20,884: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:20,885: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:20,885: INFO: training on 338503 raw words (279870 effective words) took 0.2s, 1132566 effective words/s\n",
      "2019-07-04 09:39:20,886: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:21,140: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:21,143: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:21,147: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:21,147: INFO: training on 338503 raw words (279935 effective words) took 0.3s, 1088688 effective words/s\n",
      "2019-07-04 09:39:21,149: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:21,396: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:21,400: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:21,402: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:21,402: INFO: training on 338503 raw words (280029 effective words) took 0.2s, 1128386 effective words/s\n",
      "2019-07-04 09:39:21,403: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:21,654: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:21,657: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:21,658: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:21,659: INFO: training on 338503 raw words (279906 effective words) took 0.3s, 1108430 effective words/s\n",
      "2019-07-04 09:39:21,661: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:21,916: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:21,917: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:21,918: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:21,919: INFO: training on 338503 raw words (279850 effective words) took 0.3s, 1095553 effective words/s\n",
      "2019-07-04 09:39:21,921: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:22,168: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:22,171: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:22,180: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:22,180: INFO: training on 338503 raw words (280026 effective words) took 0.3s, 1088767 effective words/s\n",
      "2019-07-04 09:39:22,182: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:22,431: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:22,434: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:22,436: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:22,437: INFO: training on 338503 raw words (279977 effective words) took 0.3s, 1109777 effective words/s\n",
      "2019-07-04 09:39:22,439: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:22,683: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:22,684: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:22,686: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:22,687: INFO: training on 338503 raw words (279867 effective words) took 0.2s, 1151037 effective words/s\n",
      "2019-07-04 09:39:22,688: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:22,931: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:22,940: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:22,945: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:22,945: INFO: training on 338503 raw words (279944 effective words) took 0.3s, 1097368 effective words/s\n",
      "2019-07-04 09:39:22,947: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:23,188: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:23,194: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:23,196: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:23,197: INFO: training on 338503 raw words (279959 effective words) took 0.2s, 1140335 effective words/s\n",
      "2019-07-04 09:39:23,198: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:23,439: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:23,445: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:23,449: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:23,450: INFO: training on 338503 raw words (280016 effective words) took 0.2s, 1126641 effective words/s\n",
      "2019-07-04 09:39:23,452: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:23,699: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:23,705: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:23,708: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:23,708: INFO: training on 338503 raw words (279991 effective words) took 0.3s, 1101529 effective words/s\n",
      "2019-07-04 09:39:23,710: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:23,956: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:23,957: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:23,965: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:23,966: INFO: training on 338503 raw words (279796 effective words) took 0.3s, 1113939 effective words/s\n",
      "2019-07-04 09:39:23,967: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:24,213: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:24,221: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:24,224: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:24,225: INFO: training on 338503 raw words (280088 effective words) took 0.3s, 1098993 effective words/s\n",
      "2019-07-04 09:39:24,226: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:24,466: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:24,470: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:24,476: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:24,476: INFO: training on 338503 raw words (279949 effective words) took 0.2s, 1139169 effective words/s\n",
      "2019-07-04 09:39:24,478: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:24,719: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:24,723: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:24,728: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:24,728: INFO: training on 338503 raw words (279849 effective words) took 0.2s, 1129783 effective words/s\n",
      "2019-07-04 09:39:24,729: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-04 09:39:24,973: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:24,982: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:24,982: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:24,983: INFO: training on 338503 raw words (279792 effective words) took 0.3s, 1114507 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# train word2vec model ; shuffle data every epoch\n",
    "for i in range(20):\n",
    "    random.shuffle(data)\n",
    "    model.train(data, total_examples=len(data), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## save model\n",
    "# model.save('/project/at081-group38/AT081193_ER/word2vec_model/CBOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load word2vec model\n",
    "# model = word2vec.Word2Vec.load('/project/at081-group38/AT081193_ER/word2vec_model/CBOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter words that not in word2vec's vocab\n",
    "data_filtered = [[w for w in l if w in model.wv] for l in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute average word vector\n",
    "avg_vector = []\n",
    "\n",
    "for l in data_filtered:\n",
    "    if len(l)==0:\n",
    "        avg_vector.append(np.array([0]*256))\n",
    "    else:\n",
    "        avg_vector.append(np.mean([model.wv[w] for w in l], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## print result\n",
    "# avg_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## save result\n",
    "# with open('/project/at081-group38/AT081193_ER/word2vec_model/'+cut_data+'avg_vector', 'wb') as file:\n",
    "#     pickle.dump(avg_vector, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load average word2vec result\n",
    "# with open('/project/at081-group38/AT081193_ER/word2vec_model/'+cut_data+'avg_vector', 'rb') as file:\n",
    "#     avg_vector = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in range(len(avg_vector)):\n",
    "    result = 1 - spatial.distance.cosine(avg_vector[0], avg_vector[i])\n",
    "    li.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_average_word_vec = pd.DataFrame(li)\n",
    "cosine_similarity_average_word_vec.columns = ['cosine_similarity']\n",
    "cosine_similarity_average_word_vec = cosine_similarity_average_word_vec.sort_values(by = ['cosine_similarity'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_candidate_average_word_vec = bert_result_with_user_last_news.iloc[cosine_similarity_average_word_vec.index[1:400],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目標不存在候選集合當中\n"
     ]
    }
   ],
   "source": [
    "validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "if final_candidate_average_word_vec[final_candidate_average_word_vec.news_guid==validation_data_guid].empty:\n",
    "    print('目標不存在候選集合當中')\n",
    "else:\n",
    "    print('目標已存在候選集合當中')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a document id map\n",
    "sentence_list = []\n",
    "\n",
    "for i, l in enumerate(data):\n",
    "    sentence_list.append(doc2vec.LabeledSentence(words=l, tags=[str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## print result\n",
    "# sentence_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define 轉換器\n",
    "model = Doc2Vec(size=256, min_count=5, window=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-04 09:39:28,440: INFO: collecting all words and their counts\n",
      "2019-07-04 09:39:28,440: INFO: PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-07-04 09:39:28,494: INFO: collected 25371 word types and 1001 unique tags from a corpus of 1001 examples and 338503 words\n",
      "2019-07-04 09:39:28,494: INFO: Loading a fresh vocabulary\n",
      "2019-07-04 09:39:28,517: INFO: min_count=5 retains 6695 unique words (26% of original 25371, drops 18676)\n",
      "2019-07-04 09:39:28,517: INFO: min_count=5 leaves 308303 word corpus (91% of original 338503, drops 30200)\n",
      "2019-07-04 09:39:28,533: INFO: deleting the raw counts dictionary of 25371 items\n",
      "2019-07-04 09:39:28,534: INFO: sample=0.001 downsamples 42 most-common words\n",
      "2019-07-04 09:39:28,534: INFO: downsampling leaves estimated 279954 word corpus (90.8% of prior 308303)\n",
      "2019-07-04 09:39:28,535: INFO: estimated required memory for 6695 words and 256 dimensions: 18284084 bytes\n",
      "2019-07-04 09:39:28,548: INFO: resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "## build vocabulary\n",
    "model.build_vocab(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-04 09:39:28,647: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:28,914: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:28,925: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:28,929: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:28,929: INFO: training on 338503 raw words (280713 effective words) took 0.3s, 1008762 effective words/s\n",
      "2019-07-04 09:39:28,930: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:29,197: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:29,209: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:29,215: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:29,216: INFO: training on 338503 raw words (280904 effective words) took 0.3s, 995009 effective words/s\n",
      "2019-07-04 09:39:29,217: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:29,484: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:29,493: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:29,495: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:29,495: INFO: training on 338503 raw words (280754 effective words) took 0.3s, 1020063 effective words/s\n",
      "2019-07-04 09:39:29,496: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:29,770: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:29,774: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:29,778: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:29,779: INFO: training on 338503 raw words (280989 effective words) took 0.3s, 1005735 effective words/s\n",
      "2019-07-04 09:39:29,780: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:30,056: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:30,057: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:30,066: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:30,067: INFO: training on 338503 raw words (280806 effective words) took 0.3s, 989212 effective words/s\n",
      "2019-07-04 09:39:30,068: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:30,346: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:30,351: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:30,353: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:30,353: INFO: training on 338503 raw words (280745 effective words) took 0.3s, 991286 effective words/s\n",
      "2019-07-04 09:39:30,354: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:30,634: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:30,637: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:30,643: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:30,643: INFO: training on 338503 raw words (280825 effective words) took 0.3s, 978635 effective words/s\n",
      "2019-07-04 09:39:30,644: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:30,908: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:30,916: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:30,918: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:30,918: INFO: training on 338503 raw words (281108 effective words) took 0.3s, 1036620 effective words/s\n",
      "2019-07-04 09:39:30,920: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:31,186: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:31,187: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:31,196: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:31,196: INFO: training on 338503 raw words (281263 effective words) took 0.3s, 1024823 effective words/s\n",
      "2019-07-04 09:39:31,198: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:31,470: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:31,471: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:31,471: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:31,472: INFO: training on 338503 raw words (280989 effective words) took 0.3s, 1037304 effective words/s\n",
      "2019-07-04 09:39:31,473: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:31,747: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:31,749: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:31,750: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:31,750: INFO: training on 338503 raw words (281007 effective words) took 0.3s, 1032169 effective words/s\n",
      "2019-07-04 09:39:31,751: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:32,016: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:32,024: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:32,030: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:32,031: INFO: training on 338503 raw words (280936 effective words) took 0.3s, 1017305 effective words/s\n",
      "2019-07-04 09:39:32,032: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:32,293: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:32,302: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:32,307: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:32,307: INFO: training on 338503 raw words (281112 effective words) took 0.3s, 1032994 effective words/s\n",
      "2019-07-04 09:39:32,309: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:32,575: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:32,580: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:32,581: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:32,581: INFO: training on 338503 raw words (280798 effective words) took 0.3s, 1040703 effective words/s\n",
      "2019-07-04 09:39:32,583: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:32,850: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:32,855: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:32,857: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:32,857: INFO: training on 338503 raw words (280993 effective words) took 0.3s, 1029628 effective words/s\n",
      "2019-07-04 09:39:32,859: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:33,125: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:33,133: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:33,139: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:33,139: INFO: training on 338503 raw words (281079 effective words) took 0.3s, 1013599 effective words/s\n",
      "2019-07-04 09:39:33,140: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:33,408: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:33,410: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:33,411: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:33,412: INFO: training on 338503 raw words (280803 effective words) took 0.3s, 1047236 effective words/s\n",
      "2019-07-04 09:39:33,413: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:33,684: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:33,686: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:33,690: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:33,690: INFO: training on 338503 raw words (281152 effective words) took 0.3s, 1020946 effective words/s\n",
      "2019-07-04 09:39:33,692: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:33,964: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:33,967: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:33,969: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:33,969: INFO: training on 338503 raw words (280910 effective words) took 0.3s, 1020986 effective words/s\n",
      "2019-07-04 09:39:33,971: INFO: training model with 3 workers on 6695 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-07-04 09:39:34,228: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-04 09:39:34,241: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-04 09:39:34,243: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-04 09:39:34,244: INFO: training on 338503 raw words (280870 effective words) took 0.3s, 1037038 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# train word2vec model ; shuffle data every epoch\n",
    "for i in range(20):\n",
    "    random.shuffle(sentence_list)\n",
    "    model.train(sentence_list, total_examples=len(data), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## print result\n",
    "# model.docvecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## save model\n",
    "# model.save('/project/at081-group38/AT081193_ER/word2vec_model/doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load word2vec model\n",
    "# model = word2vec.Word2Vec.load('/project/at081-group38/AT081193_ER/word2vec_model/doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in range(len(model.docvecs)):\n",
    "    result = 1 - spatial.distance.cosine(model.docvecs[0], model.docvecs[i])\n",
    "    li.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_doc2vec = pd.DataFrame(li)\n",
    "cosine_similarity_doc2vec.columns = ['cosine_similarity']\n",
    "cosine_similarity_doc2vec = cosine_similarity_doc2vec.sort_values(by = ['cosine_similarity'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_candidate_doc2vec = bert_result_with_user_last_news.iloc[cosine_similarity_doc2vec.index[1:400],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目標不存在候選集合當中\n"
     ]
    }
   ],
   "source": [
    "validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "if final_candidate_doc2vec[final_candidate_doc2vec.news_guid==validation_data_guid].empty:\n",
    "    print('目標不存在候選集合當中')\n",
    "else:\n",
    "    print('目標已存在候選集合當中')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_data = pd.DataFrame(content_data, columns=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word count\n",
    "## http://blog.csdn.net/gatieme/article/details/43235791 (中文正則表達式)\n",
    "content_data['word_count'] = content_data['content'].str.count('[a-zA-Z0-9]+') + content_data['content'].str.count('[\\u4e00-\\u9fff]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "## punctuation count\n",
    "content_data['punctuation'] = content_data['content'].str.replace('[\\w\\s]', '')\n",
    "content_data['punctuation_count'] = content_data['punctuation'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "## question mark count\n",
    "content_data['question_count'] = content_data['punctuation'].str.count('[?？]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop punctuation column\n",
    "content_data = content_data.drop(['punctuation'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # punctuation 是標點符號\n",
    "# content_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## compute correlation\n",
    "# content_data.iloc[:, 1:].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define transformer (轉換器)\n",
    "vectorizer = CountVectorizer()\n",
    "count = vectorizer.fit_transform([' '.join(x) for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data as pickle format\n",
    "with open('/project/at081-group38/AT081150_AN/193_ER/word2vec_model/news_count', \"wb\") as file:\n",
    "    pickle.dump([vectorizer, count], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load bag of words result\n",
    "with open('/project/at081-group38/AT081150_AN/193_ER/word2vec_model/news_count', 'rb') as file:\n",
    "    _, count = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select top 256 words (counts of document) \n",
    "most_count_id = np.array((count > 0).sum(axis=0))[0].argsort()[::-1][:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subset data\n",
    "count = count[:, most_count_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_data = np.zeros((content_data.shape[0], 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subset bag of words matrix\n",
    "count_data = count[count_data[:, 0]].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in range(len(count_data)):\n",
    "    result = 1 - spatial.distance.cosine(count_data[0], count_data[i])\n",
    "    li.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_bag_of_words = pd.DataFrame(li)\n",
    "cosine_similarity_bag_of_words.columns = ['cosine_similarity']\n",
    "cosine_similarity_bag_of_words = cosine_similarity_bag_of_words.sort_values(by = ['cosine_similarity'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_candidate_bag_of_words = bert_result_with_user_last_news.iloc[cosine_similarity_bag_of_words.index[1:400],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目標已存在候選集合當中\n"
     ]
    }
   ],
   "source": [
    "validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "if final_candidate_bag_of_words[final_candidate_bag_of_words.news_guid==validation_data_guid].empty:\n",
    "    print('目標不存在候選集合當中')\n",
    "else:\n",
    "    print('目標已存在候選集合當中')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define transformer (轉換器)\n",
    "vectorizer = TfidfVectorizer(norm=None) ## do not do normalize\n",
    "tfidf = vectorizer.fit_transform([' '.join(x) for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data as pickle format\n",
    "with open('/project/at081-group38/AT081150_AN/193_ER/word2vec_model/news_tfidf', \"wb\") as file:\n",
    "    pickle.dump([vectorizer, tfidf], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select top 10 average tf-idf of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary: id as key ; word as values\n",
    "# id2word = {v:k for k, v in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## columnwise average: words tf-idf\n",
    "# avg = tfidf.sum(axis=0) / (tfidf!=0).sum(axis=0)\n",
    "\n",
    "# ## set df < 20 as 0\n",
    "# avg[(tfidf!=0).sum(axis=0)<20] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg = np.array(avg)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## top 10 tfidf's wordID\n",
    "# most_avg_id = avg.argsort()[::-1][:10].tolist()\n",
    "# most_avg_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## print top 10 tf-idf's words\n",
    "# features = [id2word[i] for i in most_avg_id]\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load tf-idf result\n",
    "with open('/project/at081-group38/AT081150_AN/193_ER/word2vec_model/news_tfidf', 'rb') as file:\n",
    "    _, tfidf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select top 256 words (counts of document) \n",
    "most_tfidf_id = np.array((tfidf > 0).sum(axis=0))[0].argsort()[::-1][:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subset data\n",
    "tfidf = tfidf[:, most_tfidf_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data = np.zeros((content_data.shape[0], 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subset tf-idf matrix\n",
    "tfidf_data = tfidf[tfidf_data[:, 0]].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in range(len(tfidf_data)):\n",
    "    result = 1 - spatial.distance.cosine(count_data[0], count_data[i])\n",
    "    li.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_TF_IDF = pd.DataFrame(li)\n",
    "cosine_similarity_TF_IDF.columns = ['cosine_similarity']\n",
    "cosine_similarity_TF_IDF = cosine_similarity_TF_IDF.sort_values(by = ['cosine_similarity'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_candidate_TF_IDF = bert_result_with_user_last_news.iloc[cosine_similarity_TF_IDF.index[1:400],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目標已存在候選集合當中\n"
     ]
    }
   ],
   "source": [
    "validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "if final_candidate_TF_IDF[final_candidate_TF_IDF.news_guid==validation_data_guid].empty:\n",
    "    print('目標不存在候選集合當中')\n",
    "else:\n",
    "    print('目標已存在候選集合當中')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hit rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 總篩選篇數約48000筆\n",
    "total_news_num = 48000\n",
    "# 後選集合佔總篇數的百分比\n",
    "percentage = [0.25/100, 0.5/100, 1/100, 1.5/100, 2/100]\n",
    "# 候選集合數 [120, 240, 480, 720, 960]\n",
    "n = ( np.array(percentage)*total_news_num ).tolist()\n",
    "score = [1, 0.9, 0.8, 0.7, 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = []\n",
    "for i in n:\n",
    "    final_candidate_bert_feature = bert_result_with_user_last_news.iloc[cosine_similarity_bert_feature.index[1:int(i)],:]\n",
    "\n",
    "    validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "    if final_candidate_bert_feature[final_candidate_bert_feature.news_guid==validation_data_guid].empty:\n",
    "        li.append(0)\n",
    "    else:\n",
    "        li.append(1)\n",
    "        break\n",
    "    \n",
    "hit_rate_bert = sum([x*y for x,y in zip_longest(score, li, fillvalue=0)])    \n",
    "hit_rate_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = []\n",
    "for i in n:\n",
    "    final_candidate_average_word_vec = bert_result_with_user_last_news.iloc[cosine_similarity_average_word_vec.index[1:int(i)],:]\n",
    "    \n",
    "    validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "    if final_candidate_average_word_vec[final_candidate_average_word_vec.news_guid==validation_data_guid].empty:\n",
    "        li.append(0)\n",
    "    else:\n",
    "        li.append(1)\n",
    "        break\n",
    "hit_rate_average_word_vec= sum([x*y for x,y in zip_longest(score, li, fillvalue=0)])    \n",
    "hit_rate_average_word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = []\n",
    "for i in n:\n",
    "    final_candidate_doc2vec = bert_result_with_user_last_news.iloc[cosine_similarity_doc2vec.index[1:int(i)],:]\n",
    "    \n",
    "    validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "    if final_candidate_doc2vec[final_candidate_doc2vec.news_guid==validation_data_guid].empty:\n",
    "        li.append(0)\n",
    "    else:\n",
    "        li.append(1)\n",
    "        break\n",
    "hit_rate_doc2vec= sum([x*y for x,y in zip_longest(score, li, fillvalue=0)])    \n",
    "hit_rate_doc2vec   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = []\n",
    "for i in n:\n",
    "    final_candidate_bag_of_words = bert_result_with_user_last_news.iloc[cosine_similarity_bag_of_words.index[1:int(i)],:]\n",
    "    \n",
    "    validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "    if final_candidate_bag_of_words[final_candidate_bag_of_words.news_guid==validation_data_guid].empty:\n",
    "        li.append(0)\n",
    "    else:\n",
    "        li.append(1)\n",
    "        break\n",
    "hit_rate_bag_of_words = sum([x*y for x,y in zip_longest(score, li, fillvalue=0)])    \n",
    "hit_rate_bag_of_words         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = []\n",
    "for i in n:\n",
    "    final_candidate_TF_IDF = bert_result_with_user_last_news.iloc[cosine_similarity_TF_IDF.index[1:int(i)],:]\n",
    "\n",
    "    validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "    if final_candidate_TF_IDF[final_candidate_TF_IDF.news_guid==validation_data_guid].empty:\n",
    "        li.append(0)\n",
    "    else:\n",
    "        li.append(1)\n",
    "        break\n",
    "hit_rate_TF_IDF = sum([x*y for x,y in zip_longest(score, li, fillvalue=0)])    \n",
    "hit_rate_TF_IDF           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
