{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'recommender_system/target_user_id.csv' does not exist: b'recommender_system/target_user_id.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-21b5f38bde75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtarget_user_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'recommender_system/target_user_id.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mvalidation_test_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'recommender_system/validation_testing_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfinal_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'recommender_system/final_final.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'recommender_system/target_user_id.csv' does not exist: b'recommender_system/target_user_id.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "target_user_id=pd.read_csv('/python/recommender_system/target_user_id.csv')\n",
    "validation_test_data=pd.read_csv('/python/recommender_system/validation_testing_data.csv')\n",
    "final_final=pd.read_csv('/python/recommender_system/final_final.csv')\n",
    "df_news_all=pd.read_csv('/python/recommender_system/df_news_all.csv')\n",
    "\n",
    "from scipy import spatial\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import re\n",
    "import os\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import pickle\n",
    "\n",
    "from gensim.models import Doc2Vec, doc2vec\n",
    "from gensim.models import word2vec\n",
    "import random\n",
    "import logging\n",
    "\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_predict_output=pd.read_csv('/project/at081-group38/AT081193_ER/project-BERt/bert/tmp/news_output/test_output/test_results.tsv',sep='\\t')\n",
    "bert_test_data_with_target = pd.read_csv('/project/at081-group38/AT081193_ER/bert_test_data_with_target.csv')\n",
    "#取得變成columns name的資料\n",
    "temp=bert_predict_output.columns.values\n",
    "\n",
    "#轉成dataFrame\n",
    "d = {'notClick':[temp[0]], 'Click':[temp[1]]} \n",
    "first_df=pd.DataFrame(d)\n",
    "\n",
    "#將原本的欄位重新命名\n",
    "bert_predict_output.columns=['notClick','Click']\n",
    "\n",
    "#把兩個資料合併，注意 concat的第一個資料集須放變成欄位名稱的資料，不然位置會錯\n",
    "bert_predict_output=pd.concat([first_df, bert_predict_output],ignore_index=True)\n",
    "bert_predict_output.head()\n",
    "\n",
    "#把整理好的資料再跟bert_test_data水平合併\n",
    "bert_result = pd.concat([bert_test_data_with_target, bert_predict_output],axis=1)\n",
    "bert_result = bert_result.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result[[\"notClick\", \"Click\"]] = bert_result[[\"notClick\", \"Click\"]].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 這裡暫不使用\n",
    "# target_user_id=target_csv.iloc[0,:].values[0]\n",
    "# pred_time = validation_test_data[validation_test_data.user_id == target_user_id].iloc[0,:].time\n",
    "\n",
    "# bert_result_date = bert_result.date\n",
    "# for i in range(len(bert_result)):\n",
    "#     if (pred_time-bert_result_date[i])>200000000: # 若該新聞發布日期和推薦時間超過兩個月則刪除該候選新聞\n",
    "#         bert_result = bert_result.drop([i])\n",
    "# bert_result = bert_result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result = bert_result.sort_values(by=\"Click\" , ascending=False)\n",
    "bert_result = bert_result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bert_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### 選擇分數最高的前 n 筆\n",
    "n = 1000\n",
    "####################\n",
    "bert_result = bert_result.drop(bert_result.index.values[(bert_result.index.values>n-1).tolist()].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user_id_str=target_user_id.iloc[0,:].values[0]\n",
    "validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "if bert_result[bert_result.news_guid==validation_data_guid].empty:\n",
    "    print('目標不存在候選集合當中')\n",
    "\n",
    "else:\n",
    "    print('目標已存在候選集合當中')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將最後一筆觀看紀錄加入資料當中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_final = final_final.loc[:,'time':'news_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出該 user 最後一筆看的 news\n",
    "particular_user_last_news_id = final_final[final_final.user_id == target_user_id_str].tail(1).news_id.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_all.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particular_user_last_news = df_news_all[df_news_all.news_guid == particular_user_last_news_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particular_user_last_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result = bert_result.drop(['notClick', 'Click'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_with_user_last_news = pd.concat([particular_user_last_news, bert_result], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_with_user_last_news.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "content=[]\n",
    "title_content=[]\n",
    "\n",
    "for index,row in tqdm_notebook(bert_result_with_user_last_news.iterrows()):\n",
    "    title.append(row['title'])\n",
    "    content.append(row['content_remove_html_tag'])\n",
    "    title_content.append(row['title']+' '+row['content_remove_html_tag'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove html tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 處理 title 的 html\n",
    "li = []\n",
    "for i in range(len(title)):\n",
    "    li.append(\" \".join(title[i].split()))\n",
    "title = li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 處理 content 的 html\n",
    "li = []\n",
    "for i in range(len(content)):\n",
    "    li.append(\" \".join(content[i].split()))\n",
    "content = li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 處理 content 的 html\n",
    "li = []\n",
    "for i in range(len(title_content)):\n",
    "    li.append(\" \".join(title_content[i].split()))\n",
    "title_content = li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp='\\n'\n",
    "title_s=sp.join(title)\n",
    "content_s=sp.join(content)\n",
    "title_content_s=sp.join(title_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/project/at081-group38/AT081193_ER/title_s.txt\", \"w\") as text_file:\n",
    "    text_file.write(title_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/project/at081-group38/AT081193_ER/content_s.txt\", \"w\") as text_file:\n",
    "    text_file.write(content_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/project/at081-group38/AT081193_ER/title_content_s.txt\", \"w\") as text_file:\n",
    "    text_file.write(title_content_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################################################################\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layers=-1,-2,-3,-4 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python extract_features.py \\\n",
    "  --input_file=/project/at081-group38/AT081193_ER/title_content_s.txt \\\n",
    "  --output_file=tmp/output.jsonl \\\n",
    "  --vocab_file=chinese_L-12_H-768_A-12/vocab.txt \\\n",
    "  --bert_config_file=chinese_L-12_H-768_A-12/bert_config.json \\\n",
    "  --init_checkpoint=tmp/news_output/model.ckpt-18200.index \\\n",
    "  --layers=-1 \\\n",
    "  --max_seq_length=128 \\\n",
    "  --batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open('tmp/output.jsonl') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in range(len(data)):\n",
    "    result = 1 - spatial.distance.cosine(data[0]['features'][0]['layers'][0]['values'], data[i]['features'][0]['layers'][0]['values'])\n",
    "    li.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_bert_feature = pd.DataFrame(li)\n",
    "cosine_similarity_bert_feature.columns = ['cosine_similarity']\n",
    "cosine_similarity_bert_feature = cosine_similarity_bert_feature.sort_values(by = ['cosine_similarity'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_candidate_bert_feature = bert_result_with_user_last_news.iloc[cosine_similarity_bert_feature.index[1:400],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果不存在裡面的話，就把加入Validation_data\n",
    "validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "if final_candidate_bert_feature[final_candidate_bert_feature.news_guid==validation_data_guid].empty:\n",
    "    print('目標不存在候選集合當中')\n",
    "else:\n",
    "    print('目標已存在候選集合當中')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"../\") # 指定工作路徑\n",
    "# os.getcwd() # 返回當前工作路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set dictionary (can define yourself)\n",
    "jieba.set_dictionary('/project/at081-group38/AT081193_ER/jieba/dict.txt.big')\n",
    "stop_words = open('/project/at081-group38/AT081193_ER/jieba/stop_words.txt', encoding=\"utf-8\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter rules\n",
    "bert_result_with_user_last_news['content_remove_html_tag'] = bert_result_with_user_last_news['content_remove_html_tag'].str.replace('https?:\\/\\/\\S*', '')\n",
    "bert_result_with_user_last_news['content_remove_html_tag'] = bert_result_with_user_last_news['content_remove_html_tag'].replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove data\n",
    "bert_result_with_user_last_news = bert_result_with_user_last_news.dropna()\n",
    "bert_result_with_user_last_news = bert_result_with_user_last_news.reset_index(drop=True)\n",
    "bert_result_with_user_last_news['idx'] = bert_result_with_user_last_news.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_with_user_last_news.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_with_user_last_news = bert_result_with_user_last_news.drop(['path', 'news_content'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_with_user_last_news['title_content'] = bert_result_with_user_last_news[['content_remove_html_tag', 'title']].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_with_user_last_news.columns = ['date', 'news_guid', 'title', 'content', 'idx', 'title_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_with_user_last_news = bert_result_with_user_last_news.reindex(columns=['date', 'news_guid', 'title', 'content',  'title_content', 'idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_with_user_last_news.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先進行斷詞 (選擇 title、 content、 title_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可選擇 title、 content、 title_content\n",
    "content_data = bert_result_with_user_last_news['title_content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for i, text in enumerate(tqdm_notebook(content_data)):\n",
    "    line = []\n",
    "\n",
    "    for w in jieba.cut(text, cut_all=False):\n",
    "        \n",
    "        ## remove stopwords and digits\n",
    "        ## can define your own rules\n",
    "        if w not in stop_words and not bool(re.match('[0-9]+', w)):\n",
    "            line.append(w)\n",
    "\n",
    "    sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "cut_data = 'content_cutted'\n",
    "########################################################\n",
    "## save data as pickle format\n",
    "with open('/project/at081-group38/AT081193_ER/'+ cut_data, \"wb\") as file:\n",
    "    pickle.dump(sentences, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通過 logging.basicConfig 函數對日誌的輸出格式及方式做相關的配置\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load 'content_cutted'\n",
    "with open('/project/at081-group38/AT081193_ER/'+ cut_data, 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## average word vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word2vec\n",
    "# sg=0 CBOW ; sg=1 skip-gram\n",
    "model = word2vec.Word2Vec(size=256, min_count=5, window=5, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "model.build_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec model ; shuffle data every epoch\n",
    "for i in range(20):\n",
    "    random.shuffle(data)\n",
    "    model.train(data, total_examples=len(data), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## save model\n",
    "# model.save('/project/at081-group38/AT081193_ER/word2vec_model/CBOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load word2vec model\n",
    "# model = word2vec.Word2Vec.load('/project/at081-group38/AT081193_ER/word2vec_model/CBOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter words that not in word2vec's vocab\n",
    "data_filtered = [[w for w in l if w in model.wv] for l in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute average word vector\n",
    "avg_vector = []\n",
    "\n",
    "for l in data_filtered:\n",
    "    if len(l)==0:\n",
    "        avg_vector.append(np.array([0]*256))\n",
    "    else:\n",
    "        avg_vector.append(np.mean([model.wv[w] for w in l], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## print result\n",
    "# avg_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## save result\n",
    "# with open('/project/at081-group38/AT081193_ER/word2vec_model/'+cut_data+'avg_vector', 'wb') as file:\n",
    "#     pickle.dump(avg_vector, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load average word2vec result\n",
    "# with open('/project/at081-group38/AT081193_ER/word2vec_model/'+cut_data+'avg_vector', 'rb') as file:\n",
    "#     avg_vector = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in range(len(avg_vector)):\n",
    "    result = 1 - spatial.distance.cosine(avg_vector[0], avg_vector[i])\n",
    "    li.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_average_word_vec = pd.DataFrame(li)\n",
    "cosine_similarity_average_word_vec.columns = ['cosine_similarity']\n",
    "cosine_similarity_average_word_vec = cosine_similarity_average_word_vec.sort_values(by = ['cosine_similarity'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_candidate_average_word_vec = bert_result_with_user_last_news.iloc[cosine_similarity_average_word_vec.index[1:400],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "if final_candidate_average_word_vec[final_candidate_average_word_vec.news_guid==validation_data_guid].empty:\n",
    "    print('目標不存在候選集合當中')\n",
    "else:\n",
    "    print('目標已存在候選集合當中')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a document id map\n",
    "sentence_list = []\n",
    "\n",
    "for i, l in enumerate(data):\n",
    "    sentence_list.append(doc2vec.LabeledSentence(words=l, tags=[str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## print result\n",
    "# sentence_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define 轉換器\n",
    "model = Doc2Vec(size=256, min_count=5, window=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## build vocabulary\n",
    "model.build_vocab(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train word2vec model ; shuffle data every epoch\n",
    "for i in range(20):\n",
    "    random.shuffle(sentence_list)\n",
    "    model.train(sentence_list, total_examples=len(data), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## print result\n",
    "# model.docvecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## save model\n",
    "# model.save('/project/at081-group38/AT081193_ER/word2vec_model/doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load word2vec model\n",
    "# model = word2vec.Word2Vec.load('/project/at081-group38/AT081193_ER/word2vec_model/doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in range(len(model.docvecs)):\n",
    "    result = 1 - spatial.distance.cosine(model.docvecs[0], model.docvecs[i])\n",
    "    li.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_doc2vec = pd.DataFrame(li)\n",
    "cosine_similarity_doc2vec.columns = ['cosine_similarity']\n",
    "cosine_similarity_doc2vec = cosine_similarity_doc2vec.sort_values(by = ['cosine_similarity'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_candidate_doc2vec = bert_result_with_user_last_news.iloc[cosine_similarity_doc2vec.index[1:400],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "if final_candidate_doc2vec[final_candidate_doc2vec.news_guid==validation_data_guid].empty:\n",
    "    print('目標不存在候選集合當中')\n",
    "else:\n",
    "    print('目標已存在候選集合當中')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_data = pd.DataFrame(content_data, columns=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word count\n",
    "## http://blog.csdn.net/gatieme/article/details/43235791 (中文正則表達式)\n",
    "content_data['word_count'] = content_data['content'].str.count('[a-zA-Z0-9]+') + content_data['content'].str.count('[\\u4e00-\\u9fff]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## punctuation count\n",
    "content_data['punctuation'] = content_data['content'].str.replace('[\\w\\s]', '')\n",
    "content_data['punctuation_count'] = content_data['punctuation'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## question mark count\n",
    "content_data['question_count'] = content_data['punctuation'].str.count('[?？]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop punctuation column\n",
    "content_data = content_data.drop(['punctuation'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # punctuation 是標點符號\n",
    "# content_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## compute correlation\n",
    "# content_data.iloc[:, 1:].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define transformer (轉換器)\n",
    "vectorizer = CountVectorizer()\n",
    "count = vectorizer.fit_transform([' '.join(x) for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data as pickle format\n",
    "with open('/project/at081-group38/AT081193_ER/word2vec_model/news_count', \"wb\") as file:\n",
    "    pickle.dump([vectorizer, count], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load bag of words result\n",
    "with open('/project/at081-group38/AT081193_ER/word2vec_model/news_count', 'rb') as file:\n",
    "    _, count = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select top 256 words (counts of document) \n",
    "most_count_id = np.array((count > 0).sum(axis=0))[0].argsort()[::-1][:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subset data\n",
    "count = count[:, most_count_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_data = np.zeros((content_data.shape[0], 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subset bag of words matrix\n",
    "count_data = count[count_data[:, 0]].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in range(len(count_data)):\n",
    "    result = 1 - spatial.distance.cosine(count_data[0], count_data[i])\n",
    "    li.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_bag_of_words = pd.DataFrame(li)\n",
    "cosine_similarity_bag_of_words.columns = ['cosine_similarity']\n",
    "cosine_similarity_bag_of_words = cosine_similarity_bag_of_words.sort_values(by = ['cosine_similarity'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_candidate_bag_of_words = bert_result_with_user_last_news.iloc[cosine_similarity_bag_of_words.index[1:400],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "if final_candidate_bag_of_words[final_candidate_bag_of_words.news_guid==validation_data_guid].empty:\n",
    "    print('目標不存在候選集合當中')\n",
    "else:\n",
    "    print('目標已存在候選集合當中')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define transformer (轉換器)\n",
    "vectorizer = TfidfVectorizer(norm=None) ## do not do normalize\n",
    "tfidf = vectorizer.fit_transform([' '.join(x) for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data as pickle format\n",
    "with open('/project/at081-group38/AT081193_ER/word2vec_model/news_tfidf', \"wb\") as file:\n",
    "    pickle.dump([vectorizer, tfidf], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select top 10 average tf-idf of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary: id as key ; word as values\n",
    "# id2word = {v:k for k, v in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## columnwise average: words tf-idf\n",
    "# avg = tfidf.sum(axis=0) / (tfidf!=0).sum(axis=0)\n",
    "\n",
    "# ## set df < 20 as 0\n",
    "# avg[(tfidf!=0).sum(axis=0)<20] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg = np.array(avg)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## top 10 tfidf's wordID\n",
    "# most_avg_id = avg.argsort()[::-1][:10].tolist()\n",
    "# most_avg_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## print top 10 tf-idf's words\n",
    "# features = [id2word[i] for i in most_avg_id]\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load tf-idf result\n",
    "with open('/project/at081-group38/AT081193_ER/word2vec_model/news_tfidf', 'rb') as file:\n",
    "    _, tfidf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select top 256 words (counts of document) \n",
    "most_tfidf_id = np.array((tfidf > 0).sum(axis=0))[0].argsort()[::-1][:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subset data\n",
    "tfidf = tfidf[:, most_tfidf_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data = np.zeros((content_data.shape[0], 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subset tf-idf matrix\n",
    "tfidf_data = tfidf[tfidf_data[:, 0]].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in range(len(tfidf_data)):\n",
    "    result = 1 - spatial.distance.cosine(count_data[0], count_data[i])\n",
    "    li.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_TF_IDF = pd.DataFrame(li)\n",
    "cosine_similarity_TF_IDF.columns = ['cosine_similarity']\n",
    "cosine_similarity_TF_IDF = cosine_similarity_TF_IDF.sort_values(by = ['cosine_similarity'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_candidate_TF_IDF = bert_result_with_user_last_news.iloc[cosine_similarity_TF_IDF.index[1:400],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "if final_candidate_TF_IDF[final_candidate_TF_IDF.news_guid==validation_data_guid].empty:\n",
    "    print('目標不存在候選集合當中')\n",
    "else:\n",
    "    print('目標已存在候選集合當中')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hit rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 總篩選篇數約48000筆\n",
    "total_news_num = 48000\n",
    "# 後選集合佔總篇數的百分比\n",
    "percentage = [0.25/100, 0.5/100, 1/100, 1.5/100, 2/100]\n",
    "# 候選集合數 [120, 240, 480, 720, 960]\n",
    "n = ( np.array(percentage)*total_news_num ).tolist()\n",
    "score = [1, 0.9, 0.8, 0.7, 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in n:\n",
    "    final_candidate_bert_feature = bert_result_with_user_last_news.iloc[cosine_similarity_bert_feature.index[1:int(i)],:]\n",
    "\n",
    "    validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "    if final_candidate_bert_feature[final_candidate_bert_feature.news_guid==validation_data_guid].empty:\n",
    "        li.append(0)\n",
    "    else:\n",
    "        li.append(1)\n",
    "        break\n",
    "    \n",
    "hit_rate_bert = sum([x*y for x,y in zip_longest(score, li, fillvalue=0)])    \n",
    "hit_rate_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in n:\n",
    "    final_candidate_average_word_vec = bert_result_with_user_last_news.iloc[cosine_similarity_average_word_vec.index[1:int(i)],:]\n",
    "    \n",
    "    validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "    if final_candidate_average_word_vec[final_candidate_average_word_vec.news_guid==validation_data_guid].empty:\n",
    "        li.append(0)\n",
    "    else:\n",
    "        li.append(1)\n",
    "        break\n",
    "hit_rate_average_word_vec= sum([x*y for x,y in zip_longest(score, li, fillvalue=0)])    \n",
    "hit_rate_average_word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in n:\n",
    "    final_candidate_doc2vec = bert_result_with_user_last_news.iloc[cosine_similarity_doc2vec.index[1:int(i)],:]\n",
    "    \n",
    "    validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "    if final_candidate_doc2vec[final_candidate_doc2vec.news_guid==validation_data_guid].empty:\n",
    "        li.append(0)\n",
    "    else:\n",
    "        li.append(1)\n",
    "        break\n",
    "hit_rate_doc2vec= sum([x*y for x,y in zip_longest(score, li, fillvalue=0)])    \n",
    "hit_rate_doc2vec   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in n:\n",
    "    final_candidate_bag_of_words = bert_result_with_user_last_news.iloc[cosine_similarity_bag_of_words.index[1:int(i)],:]\n",
    "    \n",
    "    validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "    if final_candidate_bag_of_words[final_candidate_bag_of_words.news_guid==validation_data_guid].empty:\n",
    "        li.append(0)\n",
    "    else:\n",
    "        li.append(1)\n",
    "        break\n",
    "hit_rate_bag_of_words = sum([x*y for x,y in zip_longest(score, li, fillvalue=0)])    \n",
    "hit_rate_bag_of_words         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in n:\n",
    "    final_candidate_TF_IDF = bert_result_with_user_last_news.iloc[cosine_similarity_TF_IDF.index[1:int(i)],:]\n",
    "\n",
    "    validation_data_guid = validation_test_data[validation_test_data.user_id == target_user_id_str].iloc[0,:].guid\n",
    "    if final_candidate_TF_IDF[final_candidate_TF_IDF.news_guid==validation_data_guid].empty:\n",
    "        li.append(0)\n",
    "    else:\n",
    "        li.append(1)\n",
    "        break\n",
    "hit_rate_TF_IDF = sum([x*y for x,y in zip_longest(score, li, fillvalue=0)])    \n",
    "hit_rate_TF_IDF           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
